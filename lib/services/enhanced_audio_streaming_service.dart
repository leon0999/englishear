import 'dart:typed_data';
import 'dart:convert';
import '../utils/audio_utils.dart';
import 'dart:async';
import 'dart:io';
import 'dart:collection';
import 'package:audioplayers/audioplayers.dart';
import 'package:record/record.dart';
import 'package:audio_session/audio_session.dart' as audio_session;
import 'package:path_provider/path_provider.dart';
import 'openai_realtime_websocket.dart';
import '../core/logger.dart';

/// Enhanced Audio Streaming Service for Realtime API with Direct PCM Streaming
class EnhancedAudioStreamingService {
  final AudioRecorder _recorder = AudioRecorder();
  late AudioPlayer _audioPlayer;
  
  // Timer ê¸°ë°˜ ì˜¤ë””ì˜¤ í ì‹œìŠ¤í…œ
  final List<Uint8List> _audioQueue = [];
  bool _isPlaying = false;  // í˜„ì¬ ì¬ìƒ ì¤‘
  Timer? _processTimer;  // í ì²˜ë¦¬ íƒ€ì´ë¨¸ (500ms ì£¼ê¸°ë¡œ ë³€ê²½)
  StreamSubscription? _playerCompleteSubscription;  // ì¬ìƒ ì™„ë£Œ ë¦¬ìŠ¤ë„ˆ
  final OpenAIRealtimeWebSocket _websocket;
  Completer<void>? _playbackCompleter;  // ì¬ìƒ ì™„ë£Œ ëŒ€ê¸°ìš©
  
  // 24kHz, 16-bit PCM ì„¤ì •
  static const int sampleRate = 24000;
  static const int bytesPerSample = 2;
  static const int channels = 1;
  
  StreamSubscription? _audioStreamSubscription;
  StreamSubscription? _audioDataSubscription;
  
  bool _isRecording = false;
  bool _isSpeaking = false;
  bool _jupiterSpeaking = false;  // Jupiter AI ìŒì„± ì¬ìƒ ìƒíƒœ
  bool _aiIsResponding = false;
  bool _isInitialized = false;
  
  // Conversation history for Upgrade Replay
  final List<ConversationSegment> conversationHistory = [];
  
  // Stream controllers
  StreamController<double>? _audioLevelController;
  StreamController<ConversationState>? _conversationStateController;
  
  Stream<double> get audioLevelStream => _audioLevelController?.stream ?? const Stream.empty();
  Stream<ConversationState> get conversationStateStream => _conversationStateController?.stream ?? const Stream.empty();
  
  EnhancedAudioStreamingService(this._websocket) {
    // Initialize StreamControllers in constructor
    _audioLevelController = StreamController<double>.broadcast();
    _conversationStateController = StreamController<ConversationState>.broadcast();
    AppLogger.test('StreamControllers initialized in constructor');
    
    _setupListeners();
    
    // Set up callback for response completion
    _websocket.onResponseCompleted = () {
      AppLogger.info('ğŸ¯ Response completed - allowing AI audio playback');
      _aiIsResponding = false;
      _isSpeaking = false;  // Reset speaking state when AI completes
      AppLogger.test('Reset speaking state - AI response complete');
      _updateConversationState();
    };
  }
  
  /// Initialize service with PCM streaming support
  Future<void> initialize() async {
    AppLogger.test('==================== AUDIO SERVICE INIT START ====================');
    AppLogger.info('ğŸµ Initializing audio service...');
    
    // ì´ˆê¸° ìƒíƒœ ë¦¬ì…‹
    _isSpeaking = false;
    _aiIsResponding = false;
    _isPlaying = false;
    _isRecording = false;
    _audioQueue.clear();
    AppLogger.test('ğŸ”„ Initial state reset - all flags set to false');
    
    // AudioPlayer ì´ˆê¸°í™”
    _audioPlayer = AudioPlayer();
    await _audioPlayer.setReleaseMode(ReleaseMode.stop);
    // ìŒì„± ì†ë„ ì¡°ì ˆ (0.9 = 10% ëŠë¦¬ê²Œ)
    await _audioPlayer.setPlaybackRate(0.9);
    AppLogger.test('âœ… AudioPlayer initialized with 0.9x playback rate');
    
    // Timer ê¸°ë°˜ í ì²˜ë¦¬ ì‹œì‘ (500msë§ˆë‹¤ë¡œ ë³€ê²½ - ë” ì•ˆì •ì )
    _processTimer?.cancel();
    _processTimer = Timer.periodic(const Duration(milliseconds: 500), (_) {
      if (!_isPlaying && _audioQueue.isNotEmpty && !_isSpeaking) {
        _processQueue();
      }
    });
    AppLogger.test('âœ… Process timer started (500ms interval)');
    
    // StreamController ì¬ì´ˆê¸°í™” (ì´ë¯¸ ìƒì„±ìì—ì„œ ì´ˆê¸°í™”ë¨)
    if (_audioLevelController == null || _audioLevelController!.isClosed) {
      _audioLevelController = StreamController<double>.broadcast();
      AppLogger.test('âœ… AudioLevelController recreated');
    }
    if (_conversationStateController == null || _conversationStateController!.isClosed) {
      _conversationStateController = StreamController<ConversationState>.broadcast();
      AppLogger.test('âœ… ConversationStateController recreated');
    }
    
    try {
      // ì˜¤ë””ì˜¤ ì„¸ì…˜ ì„¤ì • - speech ì„¤ì • ì‚¬ìš© (ë” ì•ˆì •ì )
      AppLogger.test('Setting up audio session...');
      final session = await audio_session.AudioSession.instance;
      await session.configure(audio_session.AudioSessionConfiguration.speech());
      await session.setActive(true);
      AppLogger.success('Audio session configured and activated');
      
      // Request microphone permission
      AppLogger.test('Checking microphone permission...');
      final hasPermission = await _recorder.hasPermission();
      AppLogger.audio('Microphone permission status', data: {'hasPermission': hasPermission});
      if (!hasPermission) {
        AppLogger.warning('Microphone permission not granted');
        return;
      }
      AppLogger.success('Microphone permission granted');
      
      // AudioPlayer ì´ˆê¸°í™” - WAV íŒŒì¼ ì¬ìƒìš©
      AppLogger.test('Initializing AudioPlayer...');
      await _audioPlayer.setReleaseMode(ReleaseMode.stop);
      await _audioPlayer.setVolume(1.0);
      await _audioPlayer.setPlayerMode(PlayerMode.mediaPlayer);
      
      _isInitialized = true;
      
      AppLogger.success('Audio streaming ready (WAV mode)');
      AppLogger.test('==================== AUDIO SERVICE INIT COMPLETE ====================');
      
      // Auto-start continuous listening after initialization
      await Future.delayed(const Duration(seconds: 1));
      await startContinuousListening();
      
    } catch (e, stackTrace) {
      AppLogger.error('âŒ Audio init error', e, stackTrace);
      AppLogger.test('Attempting retry initialization...');
      // ì—ëŸ¬ ë°œìƒ ì‹œ ì¬ì‹œë„
      await _retryInitialize();
    }
  }
  
  Future<void> _retryInitialize() async {
    try {
      await Future.delayed(const Duration(milliseconds: 500));
      
      await _audioPlayer.setReleaseMode(ReleaseMode.stop);
      await _audioPlayer.setVolume(1.0);
      
      _isInitialized = true;
      AppLogger.info('âœ… Audio streaming ready (retry - WAV mode)');
    } catch (e) {
      AppLogger.error('âŒ Retry failed', e);
    }
  }
  
  /// Setup all event listeners
  void _setupListeners() {
    // Listen for audio data from AI
    _audioDataSubscription = _websocket.audioDataStream.listen((audioData) {
      // ë¹ˆ ë°ì´í„°ëŠ” ì¸í„°ëŸ½ì…˜ ì‹ í˜¸ë¡œ ì²˜ë¦¬
      if (audioData.isEmpty) {
        AppLogger.info('ğŸ›‘ Received interrupt signal - stopping AI audio');
        _audioPlayer.stop();
        return;
      }
      
      // AI ì˜¤ë””ì˜¤ ì¬ìƒ ì¡°ê±´ì„ ë” ìœ ì—°í•˜ê²Œ ë³€ê²½
      if (audioData.isNotEmpty) {
        AppLogger.test('ğŸ“» Received AI audio: ${audioData.length} bytes');
        AppLogger.test('   Current _isSpeaking: $_isSpeaking');
        AppLogger.test('   Current _aiIsResponding: $_aiIsResponding');
        
        // ë” ì ê·¹ì ìœ¼ë¡œ ì˜¤ë””ì˜¤ ì¬ìƒ - _isSpeakingì´ falseì¼ ë•Œ ë°”ë¡œ ì¬ìƒ
        if (!_isSpeaking) {
          AppLogger.success('[AUDIO] Playing AI audio - user not speaking');
          addAudioData(audioData);
        } else {
          AppLogger.warning('[AUDIO] Skipping AI audio - user is speaking');
          // ìƒíƒœë¥¼ ë‹¤ì‹œ í™•ì¸í•˜ê³  í•„ìš”ì‹œ ë¦¬ì…‹
          AppLogger.test('Double-checking speaking state...');
        }
      }
    });
  }
  
  /// Add audio data to play
  void addAudioData(Uint8List pcmData) {
    if (!_isInitialized) {
      AppLogger.error('âŒ [AUDIO] Player not initialized!');
      return;
    }
    
    if (pcmData.isEmpty) {
      AppLogger.error('âŒ [AUDIO] Empty PCM data received!');
      return;
    }
    
    // ì‚¬ìš©ìê°€ ë§í•˜ê³  ìˆìœ¼ë©´ ì˜¤ë””ì˜¤ ìŠ¤í‚µ
    if (_isSpeaking) {
      AppLogger.warning('âš ï¸ [AUDIO] Skipping audio - user is speaking');
      return;
    }
    
    // íì— ì¶”ê°€
    _audioQueue.add(pcmData);
    AppLogger.info('ğŸ“¦ [AUDIO] Added ${pcmData.length} bytes, queue size: ${_audioQueue.length}');
    
    // Save AI audio for conversation history
    conversationHistory.add(ConversationSegment(
      role: 'assistant',
      audioData: pcmData,
      timestamp: DateTime.now(),
    ));
  }
  
  /// Start continuous listening mode (auto-start)
  Future<void> startContinuousListening() async {
    if (_isRecording) {
      AppLogger.info('Already in continuous listening mode');
      return;
    }
    
    AppLogger.info('Starting continuous listening mode');
    await startStreaming();
  }
  
  /// Start streaming audio to Realtime API
  Future<void> startStreaming() async {
    try {
      if (_isRecording) {
        AppLogger.warning('Already recording');
        return;
      }
      
      // Stop any AI audio playback when user starts speaking
      stopListening();
      _aiIsResponding = false;
      
      // ì˜¤ë””ì˜¤ í í´ë¦¬ì–´ ë° ì¬ìƒ ì¤‘ì§€
      _audioQueue.clear();
      await _audioPlayer.stop();
      _isPlaying = false;
      
      // Check microphone permission
      if (!await _recorder.hasPermission()) {
        throw Exception('Microphone permission denied');
      }
      
      AppLogger.info('User started speaking - stopping AI response');
      _isSpeaking = true;
      _updateConversationState();
      
      // Clear any previous audio buffer
      _websocket.clearAudioBuffer();
      
      // Start recording with PCM16 format at 24kHz
      final stream = await _recorder.startStream(
        const RecordConfig(
          encoder: AudioEncoder.pcm16bits,
          sampleRate: 24000,
          numChannels: 1,
          bitRate: 128000,
        ),
      );
      
      _isRecording = true;
      final audioChunks = <int>[];
      
      // Stream audio chunks to WebSocket
      _audioStreamSubscription = stream.listen(
        (chunk) {
          final audioData = Uint8List.fromList(chunk);
          
          // Send audio chunk to Realtime API
          _websocket.sendAudio(audioData);
          
          // Save for conversation history
          audioChunks.addAll(chunk);
          
          // Also save to conversation history immediately for replay
          if (audioData.isNotEmpty) {
            conversationHistory.add(ConversationSegment(
              role: 'user',
              audioData: audioData,
              timestamp: DateTime.now(),
            ));
          }
          
          // Calculate audio level for visualization
          _calculateAudioLevel(audioData);
        },
        onError: (error) {
          AppLogger.error('Audio stream error', error);
          stopStreaming();
        },
      );
      
      AppLogger.info('Audio streaming started');
      
    } catch (e) {
      AppLogger.error('Failed to start audio streaming', e);
      _isRecording = false;
      rethrow;
    }
  }
  
  /// Stop streaming and trigger response
  Future<void> stopStreaming() async {
    try {
      if (!_isRecording) return;
      
      AppLogger.info('User stopped speaking');
      _isSpeaking = false;
      _aiIsResponding = true; // Set AI as responding
      _updateConversationState();
      
      // Cancel audio stream subscription
      await _audioStreamSubscription?.cancel();
      _audioStreamSubscription = null;
      
      // Stop recording
      await _recorder.stop();
      _isRecording = false;
      
      // Reset audio level
      if (_audioLevelController != null && !_audioLevelController!.isClosed) {
        _audioLevelController!.add(0.0);
      }
      
      // Commit audio and request response from Realtime API
      _websocket.commitAudioAndRespond();
      _aiIsResponding = true;
      _isSpeaking = false; // Ensure speaking state is false
      resumeListening();
      _updateConversationState();
      
      AppLogger.info('Audio streaming stopped, AI response requested');
      
    } catch (e) {
      AppLogger.error('Failed to stop audio streaming', e);
    }
  }
  
  /// Stop listening (pause AI audio)
  void stopListening() {
    // ë§ˆì´í¬ ì¼ì‹œì •ì§€ - AI ì˜¤ë””ì˜¤ ì¬ìƒ ì¤‘ë‹¨
    AppLogger.info('ğŸ›‘ [AUDIO] Stopping audio playback');
    _isSpeaking = true;
    _jupiterSpeaking = false;
    _audioQueue.clear();
    _audioPlayer.stop();
    _isPlaying = false;
    _playbackCompleter?.complete();
  }
  
  /// Resume listening (resume AI audio)
  void resumeListening() {
    // ë§ˆì´í¬ ì¬ê°œ - AI ì˜¤ë””ì˜¤ ì¬ìƒ ì¬ê°œ
    _isPlaying = true;
  }
  
  /// Retry audio playback with fallback
  Future<void> _retryAudioPlayback(Uint8List pcmData) async {
    AppLogger.info('ğŸ”„ Retrying audio playback...');
    
    try {
      // ì§§ì€ ì§€ì—° í›„ ì¬ì‹œë„
      await Future.delayed(const Duration(milliseconds: 100));
      
      // í”Œë ˆì´ì–´ ìƒíƒœ ì²´í¬ ë° ì¬ì´ˆê¸°í™”
      if (!_isInitialized) {
        await _retryInitialize();
      }
      
      // ì¬ì‹œë„
      await _playPCMAsWAV(pcmData);
      AppLogger.info('âœ… Audio retry successful');
    } catch (e) {
      AppLogger.error('âŒ Audio retry failed', e);
    }
  }

  /// Play PCM data as WAV file
  Future<void> _playPCMAsWAV(Uint8List pcmData) async {
    try {
      AppLogger.info('ğŸµ Starting WAV playback process...');
      
      // í”Œë ˆì´ì–´ ìƒíƒœ í™•ì¸
      if (!_isInitialized) {
        throw Exception('Audio service not initialized');
      }
      
      // ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥ í›„ ì¬ìƒ
      final tempDir = await getTemporaryDirectory();
      if (!await tempDir.exists()) {
        throw Exception('Temp directory does not exist');
      }
      
      final timestamp = DateTime.now().millisecondsSinceEpoch;
      final tempFile = File('${tempDir.path}/jupiter_$timestamp.wav');
      
      AppLogger.info('ğŸ“ Creating temp file: ${tempFile.path}');
      
      // PCMì„ WAVë¡œ ë³€í™˜ (audio_utils ì‚¬ìš©)
      final wavData = AudioUtils.pcmToWav(pcmData);
      await tempFile.writeAsBytes(wavData);
      
      // íŒŒì¼ ìƒì„± í™•ì¸
      if (!await tempFile.exists()) {
        throw Exception('Failed to create WAV file');
      }
      
      AppLogger.info('ğŸµ WAV file created: ${tempFile.path} (${wavData.length} bytes)');
      
      // ì´ì „ ì¬ìƒ ì¤‘ì§€
      try {
        await _audioPlayer.stop();
        AppLogger.info('â¹ï¸ Previous playback stopped');
      } catch (e) {
        AppLogger.warning('Could not stop previous playback: $e');
      }
      
      // ì˜¤ë””ì˜¤ ì„¸ì…˜ í™œì„±í™” í™•ì¸
      final session = await audio_session.AudioSession.instance;
      await session.setActive(true);
      
      // ìƒˆ íŒŒì¼ ì¬ìƒ ì‹œì‘
      AppLogger.info('â–¶ï¸ Starting playback...');
      await _audioPlayer.play(DeviceFileSource(tempFile.path));
      
      // ì¬ìƒ ì™„ë£Œ ë¦¬ìŠ¤ë„ˆ ì„¤ì •
      _audioPlayer.onPlayerComplete.listen((_) async {
        AppLogger.info('ğŸ Playback finished');
        // ì¬ìƒ ì™„ë£Œ í›„ íŒŒì¼ ì •ë¦¬
        try {
          if (await tempFile.exists()) {
            await tempFile.delete();
            AppLogger.info('ğŸ—‘ï¸ Temp WAV file cleaned up');
          }
        } catch (e) {
          AppLogger.error('Error deleting temp file', e);
        }
      });
      
      AppLogger.info('âœ… Jupiter voice playback started successfully');
      
    } catch (e) {
      AppLogger.error('âŒ Failed to play PCM as WAV: ${e.toString()}', e);
      
      // ìƒì„¸ ë””ë²„ê·¸ ì •ë³´
      AppLogger.error('Debug info - PCM size: ${pcmData.length}, Initialized: $_isInitialized');
      
      rethrow; // ì¬ì‹œë„ ë¡œì§ì—ì„œ ì²˜ë¦¬í•˜ë„ë¡ ì˜ˆì™¸ ì¬ì „íŒŒ
    }
  }
  
  /// Process audio queue (called by timer every 500ms)
  Future<void> _processQueue() async {
    // ì´ë¯¸ ì¬ìƒ ì¤‘ì´ê±°ë‚˜ íê°€ ë¹„ì—ˆê±°ë‚˜ ì‚¬ìš©ìê°€ ë§í•˜ê³  ìˆìœ¼ë©´ ìŠ¤í‚µ
    if (_isPlaying || _audioQueue.isEmpty || _isSpeaking) {
      return;
    }
    
    // ìµœì†Œ 8ê°œ ì²­í¬ê°€ ëª¨ì¼ ë•Œê¹Œì§€ ëŒ€ê¸° (ì¶©ë¶„í•œ ë²„í¼ë§)
    if (_audioQueue.length < 8 && _aiIsResponding) {
      return;
    }
    
    _isPlaying = true;
    _jupiterSpeaking = true;  // Jupiter ìŒì„± ì¬ìƒ ì‹œì‘
    _playbackCompleter = Completer<void>();
    
    try {
      // ì „ì²´ í ì²˜ë¦¬ (ëŠê¹€ ì—†ì´)
      final allChunks = List<Uint8List>.from(_audioQueue);
      _audioQueue.clear();
      
      int totalSize = 0;
      for (final chunk in allChunks) {
        totalSize += chunk.length;
      }
      
      if (totalSize == 0 || allChunks.isEmpty) {
        _isPlaying = false;
        _jupiterSpeaking = false;
        return;
      }
      
      AppLogger.info('ğŸµ [AUDIO] Playing: $totalSize bytes from ${allChunks.length} chunks');
      
      // ëª¨ë“  ì²­í¬ë¥¼ í•˜ë‚˜ë¡œ í•©ì¹˜ê¸°
      final combinedData = Uint8List(totalSize);
      int offset = 0;
      for (final chunk in allChunks) {
        combinedData.setRange(offset, offset + chunk.length, chunk);
        offset += chunk.length;
      }
      
      // í˜ì´ë“œ ì¸/ì•„ì›ƒ ì ìš©ìœ¼ë¡œ ë¶€ë“œëŸ¬ìš´ ì „í™˜
      _applyFadeInOut(combinedData);
      
      // WAV íŒŒì¼ ìƒì„± ë° ì¬ìƒ
      await _playWavWithProperTiming(combinedData);
      
    } catch (e) {
      AppLogger.error('âŒ [AUDIO] Playback error: $e', e);
    } finally {
      _isPlaying = false;
      _jupiterSpeaking = false;  // Jupiter ìŒì„± ì¬ìƒ ì¢…ë£Œ
      _playbackCompleter?.complete();
    }
  }
  
  /// Combine multiple chunks into single data
  Uint8List _combineChunks(List<Uint8List> chunks, int totalSize) {
    final combined = Uint8List(totalSize);
    int offset = 0;
    for (final chunk in chunks) {
      combined.setRange(offset, offset + chunk.length, chunk);
      offset += chunk.length;
    }
    return combined;
  }
  
  /// Play WAV audio with proper timing
  Future<void> _playWavWithProperTiming(Uint8List pcmData) async {
    try {
      // WAV í—¤ë” ì¶”ê°€
      final wavData = _createWavFile(pcmData);
      
      // ì„ì‹œ íŒŒì¼ ìƒì„±
      final tempDir = await getTemporaryDirectory();
      final timestamp = DateTime.now().millisecondsSinceEpoch;
      final tempFile = File('${tempDir.path}/jupiter_$timestamp.wav');
      
      await tempFile.writeAsBytes(wavData);
      
      // ì‚¬ìš©ìê°€ ë§í•˜ê¸° ì‹œì‘í–ˆìœ¼ë©´ ì¤‘ë‹¨
      if (_isSpeaking) {
        AppLogger.info('ğŸ›‘ [AUDIO] User speaking - abort playback');
        await tempFile.delete();
        return;
      }
      
      AppLogger.info('â–¶ï¸ [AUDIO] Starting playback...');
      
      // ì¬ìƒ ì™„ë£Œ ë¦¬ìŠ¤ë„ˆ ì„¤ì •
      final completer = Completer<void>();
      StreamSubscription? subscription;
      
      subscription = _audioPlayer.onPlayerComplete.listen((_) {
        completer.complete();
        subscription?.cancel();
      });
      
      // ì¬ìƒ ì‹œì‘
      await _audioPlayer.play(DeviceFileSource(tempFile.path));
      
      // ì¬ìƒ ì™„ë£Œ ëŒ€ê¸°
      await completer.future;
      
      AppLogger.info('ğŸ [AUDIO] Playback completed');
      
      // íŒŒì¼ ì‚­ì œ
      try {
        await tempFile.delete();
      } catch (e) {
        // ë¬´ì‹œ
      }
      
    } catch (e) {
      AppLogger.error('âŒ [AUDIO] WAV playback error', e);
    }
  }
  
  
  
  /// Create WAV file from PCM data with correct header
  Uint8List _createWavFile(Uint8List pcmData) {
    final wavHeader = Uint8List(44);
    final dataSize = pcmData.length;
    final fileSize = dataSize + 36;
    
    // RIFF header
    wavHeader.setRange(0, 4, 'RIFF'.codeUnits);
    wavHeader.buffer.asByteData().setUint32(4, fileSize, Endian.little);
    wavHeader.setRange(8, 12, 'WAVE'.codeUnits);
    
    // fmt chunk
    wavHeader.setRange(12, 16, 'fmt '.codeUnits);
    wavHeader.buffer.asByteData().setUint32(16, 16, Endian.little); // fmt chunk size
    wavHeader.buffer.asByteData().setUint16(20, 1, Endian.little); // PCM format
    wavHeader.buffer.asByteData().setUint16(22, channels, Endian.little);
    wavHeader.buffer.asByteData().setUint32(24, sampleRate, Endian.little);
    wavHeader.buffer.asByteData().setUint32(28, sampleRate * channels * bytesPerSample, Endian.little);
    wavHeader.buffer.asByteData().setUint16(32, channels * bytesPerSample, Endian.little);
    wavHeader.buffer.asByteData().setUint16(34, bytesPerSample * 8, Endian.little);
    
    // data chunk
    wavHeader.setRange(36, 40, 'data'.codeUnits);
    wavHeader.buffer.asByteData().setUint32(40, dataSize, Endian.little);
    
    // Combine header and PCM data
    return Uint8List.fromList([...wavHeader, ...pcmData]);
  }
  
  Uint8List _int16ToBytes(int value) {
    return Uint8List(2)..buffer.asByteData().setInt16(0, value, Endian.little);
  }
  
  Uint8List _int32ToBytes(int value) {
    return Uint8List(4)..buffer.asByteData().setInt32(0, value, Endian.little);
  }
  
  /// Apply fade in/out for smooth transitions
  void _applyFadeInOut(Uint8List data) {
    if (data.length < 960) return; // Too short for fade
    
    final fadeLength = 480; // 10ms at 24kHz
    final dataView = data.buffer.asByteData();
    
    // Fade in
    for (int i = 0; i < fadeLength && i * 2 < data.length; i++) {
      final factor = i / fadeLength;
      final sample = dataView.getInt16(i * 2, Endian.little);
      dataView.setInt16(i * 2, (sample * factor).toInt(), Endian.little);
    }
    
    // Fade out
    final startFadeOut = data.length - (fadeLength * 2);
    if (startFadeOut > 0) {
      for (int i = 0; i < fadeLength && startFadeOut + i * 2 < data.length - 1; i++) {
        final factor = 1.0 - (i / fadeLength);
        final sample = dataView.getInt16(startFadeOut + i * 2, Endian.little);
        dataView.setInt16(startFadeOut + i * 2, (sample * factor).toInt(), Endian.little);
      }
    }
  }
  
  /// Clear audio queue
  void clearQueue() {
    _audioQueue.clear();
    _isPlaying = false;
    AppLogger.info('ğŸ—‘ï¸ [AUDIO] Queue cleared');
  }
  
  /// Calculate audio level for visualization
  void _calculateAudioLevel(Uint8List chunk) {
    if (chunk.isEmpty) return;
    
    double sum = 0;
    for (int i = 0; i < chunk.length; i += 2) {
      int sample = chunk[i] | (chunk[i + 1] << 8);
      if (sample > 32767) sample = sample - 65536;
      sum += sample.abs();
    }
    
    double average = sum / (chunk.length / 2);
    double level = (average / 32768).clamp(0.0, 1.0);
    
    // null ì²´í¬ ì¶”ê°€
    if (_audioLevelController != null && !_audioLevelController!.isClosed) {
      _audioLevelController!.add(level);
    }
  }
  
  /// Update conversation state
  void _updateConversationState() {
    // null ì²´í¬ ì¶”ê°€
    if (_conversationStateController != null && !_conversationStateController!.isClosed) {
      _conversationStateController!.add(ConversationState(
        isUserSpeaking: _isSpeaking,
        isAiResponding: _aiIsResponding,
        isRecording: _isRecording,
        isPlaying: _isPlaying,
      ));
    } else {
      AppLogger.warning('ConversationStateController is null or closed');
    }
  }
  
  /// Toggle recording (press to talk)
  Future<void> toggleRecording() async {
    if (_isRecording) {
      await stopStreaming();
    } else {
      await startStreaming();
    }
  }
  
  /// Get conversation history for Upgrade Replay
  List<ConversationSegment> getConversationHistory() {
    return List.unmodifiable(conversationHistory);
  }
  
  /// Clear conversation history
  void clearConversationHistory() {
    conversationHistory.clear();
  }
  
  /// Check if currently recording
  bool get isRecording => _isRecording;
  
  /// Check if currently playing
  bool get isPlaying => _isPlaying;
  
  /// Check if user is speaking
  bool get isSpeaking => _isSpeaking;
  
  /// Check if Jupiter is speaking
  bool get isJupiterSpeaking => _jupiterSpeaking;
  
  /// Wait for playback completion
  Future<void> waitForCompletion() async {
    if (_playbackCompleter != null && !_playbackCompleter!.isCompleted) {
      await _playbackCompleter!.future;
    }
  }
  
  /// Reset speaking state (called from app lifecycle)
  void resetSpeakingState() {
    AppLogger.test('Resetting speaking state');
    _isSpeaking = false;
    _aiIsResponding = false;
    _updateConversationState();
  }
  
  /// Reset all state (called when app resumes)
  Future<void> resetState() async {
    AppLogger.test('Resetting audio service state');
    _isSpeaking = false;
    _jupiterSpeaking = false;
    _aiIsResponding = false;
    _isPlaying = false;
    _playbackCompleter?.complete();
    
    // Clear audio queue
    clearQueue();
    
    // Stop any ongoing playback
    try {
      await _audioPlayer.stop();
    } catch (e) {
      AppLogger.warning('Could not stop audio player: $e');
    }
    
    // Update conversation state
    _updateConversationState();
    
    AppLogger.success('Audio service state reset complete');
  }
  
  /// Reinitialize service (for app resume)
  Future<void> reinitialize() async {
    AppLogger.test('==================== AUDIO SERVICE REINIT START ====================');
    AppLogger.info('ğŸ”„ Reinitializing audio service...');
    
    // ìƒíƒœ ê°•ì œ ë¦¬ì…‹
    _isSpeaking = false;
    _aiIsResponding = false;
    _isPlaying = false;
    _isRecording = false;
    AppLogger.test('ğŸ”„ Force state reset - all flags set to false');
    
    // ì˜¤ë””ì˜¤ í í´ë¦¬ì–´
    clearQueue();
    
    // Timer ì¬ì‹œì‘
    _processTimer?.cancel();
    _processTimer = Timer.periodic(const Duration(milliseconds: 300), (_) {
      if (!_isPlaying && _audioQueue.isNotEmpty && !_isSpeaking) {
        _processQueue();
      }
    });
    
    // StreamController ì¬ìƒì„±
    try {
      // ê¸°ì¡´ ì»¨íŠ¸ë¡¤ëŸ¬ ì •ë¦¬
      await _audioLevelController?.close();
      await _conversationStateController?.close();
    } catch (e) {
      AppLogger.warning('Could not close existing controllers: $e');
    }
    
    // ìƒˆ ì»¨íŠ¸ë¡¤ëŸ¬ ìƒì„±
    _audioLevelController = StreamController<double>.broadcast();
    _conversationStateController = StreamController<ConversationState>.broadcast();
    AppLogger.test('âœ… StreamControllers recreated');
    
    // AudioPlayer ì¬ì´ˆê¸°í™”
    try {
      await _audioPlayer.stop();
      await _audioPlayer.dispose();
    } catch (e) {
      AppLogger.warning('Could not dispose audio player: $e');
    }
    
    // ìƒˆ AudioPlayer ìƒì„±
    _audioPlayer = AudioPlayer();
    AppLogger.test('âœ… AudioPlayer recreated');
    
    // ì˜¤ë””ì˜¤ ì„¸ì…˜ ì¬ì„¤ì • (OSStatus ì—ëŸ¬ ë°©ì§€)
    try {
      final session = await audio_session.AudioSession.instance;
      await session.setActive(false);
      await Future.delayed(const Duration(milliseconds: 100));
      await session.setActive(true);
      AppLogger.success('Audio session reactivated');
    } catch (e) {
      AppLogger.warning('âš ï¸ Audio session error (ignored): $e');
    }
    
    // Wait a bit for cleanup
    await Future.delayed(const Duration(milliseconds: 200));
    
    // Reinitialize
    await initialize();
    
    AppLogger.test('==================== AUDIO SERVICE REINIT COMPLETE ====================');
  }
  
  /// Pause streaming (for app pause)
  void pauseStreaming() {
    AppLogger.info('Pausing audio streaming');
    _isPlaying = false;
    _audioPlayer.stop();
    _processTimer?.cancel();
  }
  
  /// Dispose resources
  Future<void> dispose() async {
    await stopStreaming();
    
    await _audioStreamSubscription?.cancel();
    await _audioDataSubscription?.cancel();
    await _playerCompleteSubscription?.cancel();
    _processTimer?.cancel();
    
    await _recorder.dispose();
    
    // AudioPlayer ì •ë¦¬
    await _audioPlayer.stop();
    await _audioPlayer.dispose();
    
    await _audioLevelController?.close();
    await _conversationStateController?.close();
  }
}

/// Conversation segment for history tracking
class ConversationSegment {
  final String role;
  final Uint8List audioData;
  final DateTime timestamp;
  final String? text;
  
  ConversationSegment({
    required this.role,
    required this.audioData,
    required this.timestamp,
    this.text,
  });
}

/// Conversation state
class ConversationState {
  final bool isUserSpeaking;
  final bool isAiResponding;
  final bool isRecording;
  final bool isPlaying;
  
  ConversationState({
    required this.isUserSpeaking,
    required this.isAiResponding,
    required this.isRecording,
    required this.isPlaying,
  });
}