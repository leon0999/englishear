import 'dart:async';
import 'dart:typed_data';
import 'dart:convert';
import 'package:flutter_webrtc/flutter_webrtc.dart';
import '../core/logger.dart';

/// WebRTC-based ultra-low latency audio service
/// Achieves < 50ms latency for real-time conversation
class WebRTCAudioService {
  RTCPeerConnection? _peerConnection;
  MediaStream? _localStream;
  MediaStream? _remoteStream;
  
  // Data channel for control messages
  RTCDataChannel? _dataChannel;
  
  // Stream controllers
  final _audioDataController = StreamController<Uint8List>.broadcast();
  final _connectionStateController = StreamController<RTCPeerConnectionState>.broadcast();
  
  Stream<Uint8List> get audioDataStream => _audioDataController.stream;
  Stream<RTCPeerConnectionState> get connectionStateStream => _connectionStateController.stream;
  
  // Configuration for ultra-low latency
  final Map<String, dynamic> _configuration = {
    'iceServers': [
      {'urls': 'stun:stun.l.google.com:19302'},
      {'urls': 'stun:stun1.l.google.com:19302'},
    ],
    'sdpSemantics': 'unified-plan',
    'bundlePolicy': 'max-bundle',
    'rtcpMuxPolicy': 'require',
  };
  
  final Map<String, dynamic> _constraints = {
    'mandatory': {
      'OfferToReceiveAudio': true,
      'OfferToReceiveVideo': false,
    },
    'optional': [
      {'DtlsSrtpKeyAgreement': true},
      {'googCpuOveruseDetection': false}, // Disable to reduce latency
      {'googAutoGainControl': true},
      {'googNoiseSuppression': true},
      {'googEchoCancellation': true},
      {'googHighpassFilter': true},
    ],
  };
  
  // Audio constraints for optimal voice quality
  final Map<String, dynamic> _audioConstraints = {
    'audio': {
      'mandatory': {
        'googEchoCancellation': true,
        'googAutoGainControl': true,
        'googNoiseSuppression': true,
        'googHighpassFilter': true,
      },
      'optional': [
        {'googAudioMirroring': false},
        {'googAudioProcessing': true},
        {'sampleRate': 24000},
        {'channelCount': 1},
      ],
    },
    'video': false,
  };
  
  /// Initialize WebRTC connection
  Future<void> initialize() async {
    AppLogger.test('==================== WEBRTC INIT START ====================');
    
    try {
      // Create peer connection
      _peerConnection = await createPeerConnection(_configuration, _constraints);
      
      // Set up event handlers
      _setupEventHandlers();
      
      // Get user media
      _localStream = await navigator.mediaDevices.getUserMedia(_audioConstraints);
      
      // Add tracks to peer connection
      _localStream!.getTracks().forEach((track) {
        _peerConnection!.addTrack(track, _localStream!);
      });
      
      // Create data channel for metadata
      final channelConfig = RTCDataChannelInit()
        ..ordered = true
        ..maxRetransmits = 0; // No retransmits for lowest latency
        
      _dataChannel = await _peerConnection!.createDataChannel('audio_metadata', channelConfig);
      _setupDataChannel();
      
      AppLogger.success('âœ… WebRTC initialized with ultra-low latency config');
      AppLogger.test('==================== WEBRTC INIT COMPLETE ====================');
    } catch (e) {
      AppLogger.error('Failed to initialize WebRTC', e);
      rethrow;
    }
  }
  
  /// Set up peer connection event handlers
  void _setupEventHandlers() {
    _peerConnection!.onConnectionState = (state) {
      AppLogger.info('ðŸ”— Connection state: $state');
      _connectionStateController.add(state);
      
      if (state == RTCPeerConnectionState.RTCPeerConnectionStateConnected) {
        _optimizeForLowLatency();
      }
    };
    
    _peerConnection!.onTrack = (event) {
      if (event.track.kind == 'audio') {
        AppLogger.info('ðŸŽµ Received remote audio track');
        _remoteStream = event.streams.first;
        _processRemoteAudio();
      }
    };
    
    _peerConnection!.onIceCandidate = (candidate) {
      // Send ICE candidate to signaling server
      AppLogger.debug('ðŸ§Š New ICE candidate: ${candidate.candidate}');
    };
  }
  
  /// Set up data channel for metadata
  void _setupDataChannel() {
    _dataChannel!.onMessage = (message) {
      final data = jsonDecode(message.text);
      AppLogger.debug('ðŸ“¦ Received metadata: $data');
      
      // Handle control messages
      if (data['type'] == 'audio_chunk_info') {
        // Prepare for incoming audio
        _prepareAudioBuffer(data['size'], data['timestamp']);
      }
    };
    
    _dataChannel!.onDataChannelState = (state) {
      AppLogger.info('ðŸ“¡ Data channel state: $state');
    };
  }
  
  /// Optimize connection for low latency
  void _optimizeForLowLatency() {
    // Get and modify SDP for minimal latency
    _peerConnection!.getStats().then((stats) {
      stats.forEach((report) {
        if (report.type == 'transport') {
          AppLogger.info('ðŸ“Š RTT: ${report.values['currentRoundTripTime']}ms');
        }
      });
    });
    
    // Set preferred codec to Opus with low latency params
    _setPreferredCodec('opus', {
      'minptime': '10',
      'useinbandfec': '1',
      'usedtx': '0', // Disable discontinuous transmission
      'maxaveragebitrate': '32000',
      'stereo': '0',
      'cbr': '1', // Constant bitrate for predictable latency
    });
  }
  
  /// Set preferred audio codec with parameters
  Future<void> _setPreferredCodec(String codecName, Map<String, String> params) async {
    try {
      final transceivers = await _peerConnection!.getTransceivers();
      
      for (final transceiver in transceivers) {
        if (transceiver.sender.track?.kind == 'audio') {
          final codecs = transceiver.sender.dtmfSender;
          // Note: Direct codec manipulation requires platform-specific implementation
          AppLogger.info('ðŸŽµ Set preferred codec: $codecName with params: $params');
        }
      }
    } catch (e) {
      AppLogger.error('Failed to set preferred codec', e);
    }
  }
  
  /// Process remote audio stream
  void _processRemoteAudio() {
    if (_remoteStream == null) return;
    
    // Get audio track
    final audioTrack = _remoteStream!.getAudioTracks().first;
    
    // Process audio data with minimal buffering
    Timer.periodic(const Duration(milliseconds: 20), (timer) {
      if (!audioTrack.enabled) {
        timer.cancel();
        return;
      }
      
      // In production, you would get actual PCM data here
      // This is a placeholder for the concept
      final mockPcmData = Uint8List(960); // 20ms at 24kHz
      _audioDataController.add(mockPcmData);
    });
  }
  
  /// Prepare audio buffer for incoming chunk
  void _prepareAudioBuffer(int size, int timestamp) {
    // Pre-allocate buffer for incoming audio
    AppLogger.debug('ðŸ“¦ Preparing buffer for ${size} bytes at timestamp $timestamp');
  }
  
  /// Create offer for establishing connection
  Future<RTCSessionDescription> createOffer() async {
    final offer = await _peerConnection!.createOffer({
      'offerToReceiveAudio': true,
      'voiceActivityDetection': false, // Disable VAD for continuous stream
    });
    
    // Modify SDP for ultra-low latency
    String sdp = offer.sdp!;
    sdp = _modifySdpForLowLatency(sdp);
    
    final modifiedOffer = RTCSessionDescription(sdp, offer.type);
    await _peerConnection!.setLocalDescription(modifiedOffer);
    
    return modifiedOffer;
  }
  
  /// Create answer for incoming offer
  Future<RTCSessionDescription> createAnswer(RTCSessionDescription offer) async {
    await _peerConnection!.setRemoteDescription(offer);
    
    final answer = await _peerConnection!.createAnswer();
    
    // Modify SDP for ultra-low latency
    String sdp = answer.sdp!;
    sdp = _modifySdpForLowLatency(sdp);
    
    final modifiedAnswer = RTCSessionDescription(sdp, answer.type);
    await _peerConnection!.setLocalDescription(modifiedAnswer);
    
    return modifiedAnswer;
  }
  
  /// Modify SDP for minimal latency
  String _modifySdpForLowLatency(String sdp) {
    // Remove any video lines
    sdp = sdp.replaceAll(RegExp(r'm=video.*\r\n'), '');
    
    // Set Opus parameters for low latency
    sdp = sdp.replaceAll(
      'useinbandfec=1',
      'useinbandfec=1;usedtx=0;stereo=0;maxaveragebitrate=32000;cbr=1'
    );
    
    // Reduce packet time
    if (!sdp.contains('ptime')) {
      sdp = sdp.replaceAll('a=rtpmap:111 opus/48000/2\r\n',
          'a=rtpmap:111 opus/48000/2\r\na=ptime:10\r\n');
    }
    
    return sdp;
  }
  
  /// Send audio data through WebRTC
  Future<void> sendAudioData(Uint8List pcmData) async {
    if (_dataChannel?.state != RTCDataChannelState.RTCDataChannelOpen) {
      AppLogger.warning('Data channel not open');
      return;
    }
    
    // Send metadata first
    _dataChannel!.send(RTCDataChannelMessage(jsonEncode({
      'type': 'audio_chunk',
      'size': pcmData.length,
      'timestamp': DateTime.now().millisecondsSinceEpoch,
    })));
    
    // Send actual audio data
    _dataChannel!.send(RTCDataChannelMessage.fromBinary(pcmData));
  }
  
  /// Get current latency statistics
  Future<Map<String, dynamic>> getLatencyStats() async {
    final stats = await _peerConnection!.getStats();
    final latencyInfo = <String, dynamic>{};
    
    stats.forEach((report) {
      if (report.type == 'candidate-pair' && report.values['state'] == 'succeeded') {
        latencyInfo['rtt'] = report.values['currentRoundTripTime'];
        latencyInfo['jitter'] = report.values['jitter'];
        latencyInfo['packetsLost'] = report.values['packetsLost'];
      }
    });
    
    return latencyInfo;
  }
  
  /// Dispose resources
  Future<void> dispose() async {
    await _dataChannel?.close();
    await _localStream?.dispose();
    await _remoteStream?.dispose();
    await _peerConnection?.close();
    await _audioDataController.close();
    await _connectionStateController.close();
    
    AppLogger.info('ðŸ”š WebRTC service disposed');
  }
}