// lib/services/voice_conversation_service.dart

import 'dart:convert';
import 'dart:async';
import 'dart:html' as html;
import 'dart:math' as math;
import 'package:flutter/foundation.dart' show kIsWeb;
import 'package:dio/dio.dart';
import 'package:flutter_dotenv/flutter_dotenv.dart';
import 'package:speech_to_text/speech_to_text.dart' as stt;

// ëŒ€í™” í„´ ëª¨ë¸
class ConversationTurn {
  final String speaker; // 'AI' or 'User'
  final String text;
  final DateTime timestamp;
  final String? improvedVersion; // Upgrade Replayì‹œ ê°œì„ ëœ ë²„ì „

  ConversationTurn({
    required this.speaker,
    required this.text,
    required this.timestamp,
    this.improvedVersion,
  });

  Map<String, dynamic> toJson() => {
    'speaker': speaker,
    'text': text,
    'timestamp': timestamp.toIso8601String(),
    'improvedVersion': improvedVersion,
  };
}

// ì‹œë‚˜ë¦¬ì˜¤ ëª¨ë¸
class ConversationScenario {
  final String title;
  final String description;
  final String context;
  final String openingLine;

  ConversationScenario({
    required this.title,
    required this.description,
    required this.context,
    required this.openingLine,
  });
}

class VoiceConversationService {
  late final Dio _dio;
  late final String _apiKey;
  final stt.SpeechToText speechToText = stt.SpeechToText();
  
  List<ConversationTurn> conversationHistory = [];
  ConversationScenario? currentScenario;
  int turnCount = 0;
  bool isListening = false;
  bool isProcessing = false;
  
  // ìŒì„± ëª©ë¡
  static const List<String> aiVoices = ['nova', 'alloy', 'echo', 'fable', 'onyx', 'shimmer'];
  String currentAIVoice = 'nova';
  
  // ì½œë°±
  Function(String)? onAISpeaking;
  Function(String)? onUserSpeaking;
  Function(bool)? onListeningStateChanged;
  Function()? onConversationEnd;
  Function(String)? onError;
  
  VoiceConversationService() {
    _apiKey = dotenv.env['OPENAI_API_KEY'] ?? '';
    _dio = Dio(BaseOptions(
      baseUrl: 'https://api.openai.com/v1',
      connectTimeout: const Duration(seconds: 30),
      receiveTimeout: const Duration(seconds: 30),
      headers: {
        'Authorization': 'Bearer $_apiKey',
        'Content-Type': 'application/json',
      },
    ));
  }
  
  // 1. ëŒ€í™” ì‹œì‘ - GPTê°€ ìƒí™© ì„¤ì •í•˜ê³  ë¨¼ì € ë§ê±¸ê¸°
  Future<void> startConversation() async {
    try {
      // ì´ˆê¸°í™”
      conversationHistory.clear();
      turnCount = 0;
      
      // ëœë¤ ì‹œë‚˜ë¦¬ì˜¤ ìƒì„±
      currentScenario = await _generateRandomScenario();
      print('ğŸ­ Scenario: ${currentScenario!.title}');
      print('ğŸ“ Context: ${currentScenario!.description}');
      
      // ëœë¤ ìŒì„± ì„ íƒ
      currentAIVoice = aiVoices[math.Random().nextInt(aiVoices.length)];
      
      // GPT ì²« ì¸ì‚¬ (ì‹œë‚˜ë¦¬ì˜¤ ê¸°ë°˜)
      final firstMessage = currentScenario!.openingLine;
      
      // ëŒ€í™” ê¸°ë¡ì— ì¶”ê°€
      conversationHistory.add(ConversationTurn(
        speaker: 'AI',
        text: firstMessage,
        timestamp: DateTime.now(),
      ));
      
      // ê³ í’ˆì§ˆ ìŒì„±ìœ¼ë¡œ ì¬ìƒ
      await _speakWithPremiumVoice(firstMessage);
      
      // ì‚¬ìš©ì ìŒì„± ì¸ì‹ ì‹œì‘
      await _startListening();
      
    } catch (e) {
      print('âŒ Error starting conversation: $e');
      onError?.call('Failed to start conversation: $e');
    }
  }
  
  // 2. ëœë¤ ì‹œë‚˜ë¦¬ì˜¤ ìƒì„± (ê°œì„ ëœ ë²„ì „)
  Future<ConversationScenario> _generateRandomScenario() async {
    try {
      final response = await _dio.post(
        '/chat/completions',
        data: {
          'model': 'gpt-4-turbo-preview',
          'messages': [
            {
              'role': 'system',
              'content': '''You're creating realistic conversation scenarios for English practice.
              Generate a specific, interesting everyday situation where two people might talk.
              Make it natural and engaging.
              
              Examples:
              - Ordering coffee with dietary restrictions
              - Asking for tech support about slow wifi
              - Chatting with a colleague about weekend plans
              - Returning an item at a store
              - Making small talk in an elevator
              - Discussing a movie with a friend
              
              Return JSON format:
              {
                "title": "short title",
                "description": "2-3 sentence context",
                "context": "your role and personality for this conversation",
                "openingLine": "natural first line to start the conversation"
              }'''
            },
            {
              'role': 'user',
              'content': 'Generate a random everyday conversation scenario.'
            }
          ],
          'temperature': 0.9,
          'response_format': {'type': 'json_object'},
        },
      );
      
      final content = response.data['choices'][0]['message']['content'];
      final json = jsonDecode(content);
      
      return ConversationScenario(
        title: json['title'] ?? 'Casual Chat',
        description: json['description'] ?? 'A friendly conversation',
        context: json['context'] ?? 'You are having a casual conversation',
        openingLine: json['openingLine'] ?? 'Hey! How are you doing today?',
      );
      
    } catch (e) {
      print('Error generating scenario: $e');
      // í´ë°± ì‹œë‚˜ë¦¬ì˜¤
      return ConversationScenario(
        title: 'Coffee Shop',
        description: 'You\'re a barista at a busy coffee shop.',
        context: 'Friendly barista taking orders and making small talk',
        openingLine: 'Good morning! What can I get started for you today?',
      );
    }
  }
  
  // 3. ê³ í’ˆì§ˆ ìŒì„± ì¶œë ¥ (OpenAI TTS HD)
  Future<void> _speakWithPremiumVoice(String text) async {
    try {
      onAISpeaking?.call(text);
      
      final response = await _dio.post(
        '/audio/speech',
        options: Options(responseType: ResponseType.bytes),
        data: {
          'model': 'tts-1-hd',
          'voice': currentAIVoice,
          'input': text,
          'speed': 0.95,
        },
      );
      
      if (kIsWeb) {
        // ì›¹ì—ì„œ ì¬ìƒ
        final bytes = response.data;
        final blob = html.Blob([bytes]);
        final url = html.Url.createObjectUrlFromBlob(blob);
        
        final audio = html.AudioElement()
          ..src = url
          ..autoplay = false;
        
        await audio.play();
        await audio.onEnded.first;
        html.Url.revokeObjectUrl(url);
      } else {
        // ëª¨ë°”ì¼ì—ì„œëŠ” audioplayers ì‚¬ìš©
        // TODO: Implement mobile audio playback
      }
      
    } catch (e) {
      print('âŒ TTS Error: $e');
      // TTS ì‹¤íŒ¨ì‹œ ë¬´ì‹œí•˜ê³  ì§„í–‰
    }
  }
  
  // 4. ìŒì„± ì¸ì‹ ì‹œì‘
  Future<void> _startListening() async {
    if (isListening) return;
    
    try {
      bool available = await speechToText.initialize(
        onStatus: (status) {
          print('Speech recognition status: $status');
          if (status == 'done' || status == 'notListening') {
            isListening = false;
            onListeningStateChanged?.call(false);
          }
        },
        onError: (error) {
          print('Speech recognition error: $error');
          isListening = false;
          onListeningStateChanged?.call(false);
        },
      );
      
      if (available) {
        isListening = true;
        onListeningStateChanged?.call(true);
        
        await speechToText.listen(
          onResult: (result) async {
            if (result.finalResult && result.recognizedWords.isNotEmpty) {
              final userText = result.recognizedWords;
              onUserSpeaking?.call(userText);
              
              // ëŒ€í™” ê¸°ë¡ì— ì¶”ê°€
              conversationHistory.add(ConversationTurn(
                speaker: 'User',
                text: userText,
                timestamp: DateTime.now(),
              ));
              
              turnCount++;
              
              // ìŒì„± ì¸ì‹ ì¤‘ì§€
              await stopListening();
              
              // GPT ì‘ë‹µ ìƒì„±
              await _generateAIResponse(userText);
            }
          },
          listenFor: const Duration(seconds: 30),
          pauseFor: const Duration(seconds: 3),
          partialResults: true,
          localeId: 'en-US',
        );
      }
    } catch (e) {
      print('âŒ Error starting speech recognition: $e');
      onError?.call('Microphone access failed');
    }
  }
  
  // 5. ìŒì„± ì¸ì‹ ì¤‘ì§€
  Future<void> stopListening() async {
    if (isListening) {
      await speechToText.stop();
      isListening = false;
      onListeningStateChanged?.call(false);
    }
  }
  
  // 6. AI ì‘ë‹µ ìƒì„± ë° ì¬ìƒ
  Future<void> _generateAIResponse(String userInput) async {
    if (isProcessing) return;
    isProcessing = true;
    
    try {
      // ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
      final messages = [
        {
          'role': 'system',
          'content': '''You're having a natural conversation.
          Scenario: ${currentScenario?.title ?? 'Casual chat'}
          Context: ${currentScenario?.context ?? 'Be friendly and natural'}
          
          Guidelines:
          - Respond naturally like a real person would
          - Keep responses concise (1-2 sentences usually)
          - React to what they said, don't correct their grammar
          - Use casual expressions and contractions
          - Show emotion and personality
          - Include filler words occasionally (um, well, you know, I mean)
          - Ask follow-up questions to keep conversation flowing
          - After 6-8 turns, naturally wrap up the conversation'''
        },
        // ëŒ€í™” íˆìŠ¤í† ë¦¬ ì¶”ê°€
        ...conversationHistory.map((turn) => {
          'role': turn.speaker == 'AI' ? 'assistant' : 'user',
          'content': turn.text,
        }),
      ];
      
      final response = await _dio.post(
        '/chat/completions',
        data: {
          'model': 'gpt-4-turbo-preview',
          'messages': messages,
          'temperature': 0.8,
          'max_tokens': 150,
        },
      );
      
      final aiResponse = response.data['choices'][0]['message']['content'];
      
      // ëŒ€í™” ê¸°ë¡ì— ì¶”ê°€
      conversationHistory.add(ConversationTurn(
        speaker: 'AI',
        text: aiResponse,
        timestamp: DateTime.now(),
      ));
      
      // AI ì‘ë‹µ ì¬ìƒ
      await _speakWithPremiumVoice(aiResponse);
      
      // ëŒ€í™” ì¢…ë£Œ ì²´í¬ (6í„´ ì´ìƒ ë˜ëŠ” ì¢…ë£Œ ì‹ í˜¸)
      if (turnCount >= 6 || _isConversationEnding(aiResponse)) {
        onConversationEnd?.call();
      } else {
        // ê³„ì† ë“£ê¸°
        await _startListening();
      }
      
    } catch (e) {
      print('âŒ Error generating AI response: $e');
      onError?.call('Failed to generate response');
    } finally {
      isProcessing = false;
    }
  }
  
  // 7. ëŒ€í™” ì¢…ë£Œ ì‹ í˜¸ ì²´í¬
  bool _isConversationEnding(String message) {
    final endings = [
      'bye', 'goodbye', 'see you', 'talk to you later',
      'have a great', 'take care', 'gotta go', 'nice talking'
    ];
    
    final lowerMessage = message.toLowerCase();
    return endings.any((ending) => lowerMessage.contains(ending));
  }
  
  // 8. ëŒ€í™” ë‚´ì—­ ê°€ì ¸ì˜¤ê¸°
  List<ConversationTurn> getConversationHistory() {
    return List.from(conversationHistory);
  }
  
  // 9. ëŒ€í™” ì¢…ë£Œ
  void endConversation() {
    stopListening();
    onConversationEnd?.call();
  }
}